{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\test-1\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py:123: PkgResourcesDeprecationWarning: 4.0.0-unsupported is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import pytorch_lightning as pl\n",
    "from argparse import Namespace\n",
    "from model import DTSModel\n",
    "from datamodules.csvdatamodule import CsvDataModule\n",
    "from datamodules.hivedatamodule import HiveDataModule\n",
    "from datamodules.sqldatamodule import SqlDataModule\n",
    "from datamodules.s3datamodule import S3DataModule\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    data: dict\n",
    "    model: dict\n",
    "    train:dict\n",
    "        \n",
    "config = Config(\n",
    "    data= {\n",
    "        \"data\": \"csv\",\n",
    "        \"data_params\": {\n",
    "            \"num_feature\": 5,\n",
    "            \"seq_len\": 30,\n",
    "            \"tgt_len\": 1,\n",
    "            \"batch_size\": 32,\n",
    "            \"train_path\": \"data/AAPL_train.csv\",\n",
    "            \"val_path\": \"data/AAPL_val.csv\",\n",
    "            \"test_path\": \"data/AAPL_test.csv\"\n",
    "        }\n",
    "    },\n",
    "    model= {\n",
    "        \"model_name\": \"custom_rnn\",\n",
    "        \"model_params\": {\n",
    "            \"input_size\": 5,\n",
    "            \"hidden_size\": 32,\n",
    "            \"output_size\": 1,\n",
    "            \"num_layers\": 2,\n",
    "            \"lr\": 2e-5,\n",
    "            \"batch_size\": 32\n",
    "        },\n",
    "        \"loss_fn_type\": \"mse\",\n",
    "        \"loss_params\": {}\n",
    "    },\n",
    "    train=  {\n",
    "        \"accelerator\": \"auto\",\n",
    "        \"devices\": 4,\n",
    "        \"strategy\": 'ddp',\n",
    "        \"max_epochs\": 1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(42, workers=True)\n",
    "logger = TensorBoardLogger('logs/', name=config.model['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LightningDataModule\n",
    "data_module = CsvDataModule(config.data['data_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = data_module.train_dataloader()\n",
    "batch = next(iter(data_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LightningModule\n",
    "model = DTSModel(config.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DTSModel(\n",
       "  (model): CustomRNN(\n",
       "    (rnn): RNN(5, 32, num_layers=2, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       "  (loss_fn): MSE()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "MisconfigurationException",
     "evalue": "`Trainer(strategy='ddp')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible strategies: `Fabric(strategy=None|'dp'|'ddp_notebook')`. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-7ae17f4d23a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# Create Trainer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m trainer = pl.Trainer(\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0maccelerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accelerator'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mdevices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'devices'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\argparse.py\u001b[0m in \u001b[0;36minsert_env_defaults\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;31m# all args were already moved to kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_T\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minsert_env_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, logger, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, inference_mode)\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_connector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataConnector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmultiple_trainloader_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m         self._accelerator_connector = AcceleratorConnector(\n\u001b[0m\u001b[0;32m    421\u001b[0m             \u001b[0mnum_processes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_processes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m             \u001b[0mdevices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, amp_type, amp_level, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, auto_select_gpus, num_processes, tpu_cores, ipus, gpus)\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m         \u001b[1;31m# 6. Instantiate Strategy - Part 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy_init_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_init_deterministic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_LITERAL_WARN\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py\u001b[0m in \u001b[0;36m_lazy_init_strategy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    839\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_IS_INTERACTIVE\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlauncher\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_interactive_compatible\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             raise MisconfigurationException(\n\u001b[0m\u001b[0;32m    842\u001b[0m                 \u001b[1;34mf\"`Trainer(strategy={self.strategy.strategy_name!r})` is not compatible with an interactive\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m                 \u001b[1;34m\" environment. Run your code as a script, or choose one of the compatible strategies:\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMisconfigurationException\u001b[0m: `Trainer(strategy='ddp')` is not compatible with an interactive environment. Run your code as a script, or choose one of the compatible strategies: `Fabric(strategy=None|'dp'|'ddp_notebook')`. In case you are spawning processes yourself, make sure to include the Trainer creation inside the worker function."
     ]
    }
   ],
   "source": [
    "# Callback to save the model checkpoint\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        dirpath='./checkpoints/',\n",
    "        filename='model-{epoch:02d}-{val_loss:.2f}',\n",
    "        save_top_k=3,\n",
    "        mode='min',\n",
    "        save_last=True,\n",
    "        every_n_epochs=1  # Save checkpoint every 100 steps\n",
    "    )\n",
    "\n",
    "# Create Trainer\n",
    "trainer = pl.Trainer(\n",
    "        accelerator=config.train['accelerator'],\n",
    "        devices=config.train['devices'],\n",
    "        strategy=config.train['strategy'],\n",
    "        max_epochs=config.train['max_epochs'],\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor='val_loss'),\n",
    "            LearningRateMonitor(logging_interval='step'),\n",
    "            checkpoint_callback\n",
    "        ],\n",
    "        logger=logger,\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
